{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb 정보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에 자기가 import한게 없으면 추가할것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 구글 검색 사이트에서 'IMDb URL' 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일을 읽어옵니다.\n",
    "just = pd.read_excel('../data/justwatch.xlsx', index_col=0)\n",
    "just.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_result(query):\n",
    "    # NaN 값인 경우 처리합니다.\n",
    "    if pd.isna(query):\n",
    "        return None\n",
    "    \n",
    "    # 검색 쿼리를 URL 인코딩합니다.\n",
    "    query = urllib.parse.quote(query + \" TV IMDb\")\n",
    "    \n",
    "    # 구글 검색 URL을 구성합니다.\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # 요청 헤더를 설정합니다.\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # 구글 검색 페이지에 요청을 보냅니다.\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # 응답의 상태 코드를 확인합니다.\n",
    "    if response.status_code == 200:\n",
    "        # HTML 내용을 파싱합니다.\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # 모든 검색 결과를 가져옵니다.\n",
    "        search_results = soup.find_all(\"div\", class_=\"g\")\n",
    "        \n",
    "        # IMDb 사이트의 URL을 포함한 첫 번째 검색 결과를 찾습니다.\n",
    "        for result in search_results:\n",
    "            link = result.find(\"a\")\n",
    "            if link and 'href' in link.attrs:\n",
    "                url = link['href']\n",
    "                # IMDb 사이트의 URL인지 확인합니다.\n",
    "                if \"imdb.com\" in url:\n",
    "                    title = result.find(\"h3\").text\n",
    "                    return {\"title\": title, \"url\": url}\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 df에 다른 컬럼에 넣기\n",
    "def get_imdb_results(df):\n",
    "    imdb_titles = []\n",
    "    imdb_urls = []\n",
    "    for i, row in df.iterrows():\n",
    "        query = str(row[\"original_title\"]) + str(row[\"title\"]) + str(row[\"year\"])  # NaN 값이 있을 수 있으므로 문자열로 변환합니다.\n",
    "        result = get_imdb_result(query)\n",
    "        if result:\n",
    "            imdb_titles.append(result[\"title\"])\n",
    "            imdb_urls.append(result[\"url\"])\n",
    "        else:\n",
    "            imdb_titles.append(None)\n",
    "            imdb_urls.append(None)\n",
    "    return imdb_titles, imdb_urls\n",
    "\n",
    "# IMDb 제목과 URL을 가져와서 just 데이터프레임에 새로운 열로 추가합니다.\n",
    "just[\"IMDb_title\"], just[\"IMDb_URL\"] = get_imdb_results(just)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "\n",
    "just.to_excel('../data/justwatch_url.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 구글 검색 특성상 개인화?가 잘 되어있어 원하는 url를 모두 가져오기 힘듬 \n",
    "- 일단 가져온 뒤 수기로 다 확인해서 잘못된 url은 직접 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 드라마 기본 페이지 크롤링(total_rate, user_review 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일을 읽어옵니다.\n",
    "df = pd.read_excel('../data/justwatch_url.xlsx', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for url in df['IMDb_URL']:\n",
    "    # 크롬 드라이버 설정 및 페이지 열기\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # 첫 번째 페이지에서 디테일 데이터 수집 \n",
    "        # 페이지 소스 가져오기\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 디테일 정보 가져오기 \n",
    "        title_i = soup.select_one('div.sc-491663c0-3.bdjVSf > div.sc-b7c53eda-0.dUpRPQ > h1 > span').text.strip() if soup.select_one('div.sc-491663c0-3.bdjVSf > div.sc-b7c53eda-0.dUpRPQ > h1 > span') else 'N/A'\n",
    "        original_title_o = soup.select_one('div.sc-b7c53eda-0.dUpRPQ > div').text.strip() if soup.select_one('div.sc-b7c53eda-0.dUpRPQ > div') else 'N/A'\n",
    "        original_title_i = original_title_o.replace('Original title: ', '')\n",
    "        year_i = soup.select_one('div.sc-b7c53eda-0.dUpRPQ > ul > li:nth-child(2) > a').text.strip() if soup.select_one('div.sc-b7c53eda-0.dUpRPQ > ul > li:nth-child(2) > a') else 'N/A'\n",
    "        \n",
    "        total_rate = soup.select_one('div.sc-bde20123-2.cdQqzc > span.sc-bde20123-1.cMEQkK').text.strip() if soup.select_one('div.sc-bde20123-2.cdQqzc > span.sc-bde20123-1.cMEQkK') else 'N/A'\n",
    "        \n",
    "        tc = soup.select_one('div.sc-bde20123-0.dLwiNw > div.sc-bde20123-3.gPVQxL').text.strip() if soup.select_one('div.sc-bde20123-0.dLwiNw > div.sc-bde20123-3.gPVQxL') else 'N/A'\n",
    "        total_count = convert_to_number(tc)\n",
    "\n",
    "        add_to_watchlist = soup.select_one('button.ipc-split-button__btn > div > div.sc-b23676b3-3.bRgISf').text.strip() if soup.select_one('button.ipc-split-button__btn > div > div.sc-b23676b3-3.bRgISf') else 0\n",
    "        wl = add_to_watchlist.split('명의')[0] if isinstance(add_to_watchlist, str) else '0'\n",
    "        wl = str(wl)\n",
    "        watchlist = convert_to_number(wl)\n",
    "        \n",
    "        popularity = soup.select_one('div.sc-5f7fb5b4-0.brylPD > div.sc-5f7fb5b4-1.fTREEx').text.strip() if soup.select_one('div.sc-5f7fb5b4-0.brylPD > div.sc-5f7fb5b4-1.fTREEx') else 0\n",
    "        \n",
    "        ur = soup.select_one('div.sc-491663c0-11.cvvyMK > ul > li:nth-child(1) > a > span > span.score').text.strip() if soup.select_one('div.sc-491663c0-11.cvvyMK > ul > li:nth-child(1) > a > span > span.score') else 0\n",
    "        if ur == 0:\n",
    "            ur =soup.select_one('div.sc-491663c0-11.bmRzqx > ul > li > a > span > span.score').text.strip() if soup.select_one('div.sc-491663c0-11.bmRzqx > ul > li > a > span > span.score') else 0\n",
    "        user_review = convert_to_number(ur)\n",
    "\n",
    "\n",
    "        critic_review = soup.select_one('div.sc-491663c0-11.cvvyMK > ul > li:nth-child(2) > a > span > span.score').text.strip() if soup.select_one('div.sc-491663c0-11.cvvyMK > ul > li:nth-child(2) > a > span > span.score') else 0\n",
    "     \n",
    "        age_miss = soup.select_one('div.sc-491663c0-3.bdjVSf > div.sc-b7c53eda-0.dUpRPQ > ul > li:nth-child(3) > a').text.strip() if soup.select_one('div.sc-491663c0-3.bdjVSf > div.sc-b7c53eda-0.dUpRPQ > ul > li:nth-child(3) > a') else 'N/A'\n",
    "\n",
    "        data.append({\n",
    "            'title_i': title_i,\n",
    "            'original_title_i': original_title_i,\n",
    "            'year_i': year_i,\n",
    "            'total_rate': total_rate,\n",
    "            'total_count': total_count,\n",
    "            'watchlist': watchlist,\n",
    "            'popularity': popularity,\n",
    "            'user_review': user_review,\n",
    "            'critic_review': critic_review,\n",
    "            'age_miss': age_miss,\n",
    "        })\n",
    "        print(f\"Scraped: {title_i}, {original_title_i}, {year_i}, {total_rate}, {total_count}, {watchlist}, {popularity}, {user_review}, {critic_review}, {age_miss}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing URL {url}: {e}\")\n",
    "        data.append({\n",
    "            'title_i': 'N/A',\n",
    "            'original_title_i': 'N/A',\n",
    "            'year_i': 'N/A',\n",
    "            'total_rate': 0,\n",
    "            'total_count': 0,\n",
    "            'watchlist': 0,\n",
    "            'popularity': 0,\n",
    "            'user_review': 0,\n",
    "            'critic_review': 0,\n",
    "            'age_miss': 'N/A',\n",
    "        })\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. 드라마 제작사, 배급사 페이지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제작사, 배급사 크롤링\n",
    "for idx, row in tqdm_notebook(drama.iterrows()):\n",
    "\n",
    "    # 집합 만들기\n",
    "    production_set = set(); distributor_set = set()\n",
    "    production_text = '';  production_list = ''; production_companies = ''\n",
    "    distributor_text = ''; distributor_list = ''; distributor_companies = ''\n",
    "\n",
    "    # url 오류 확인\n",
    "    try:\n",
    "        page = row['IMDb_URL'] + 'companycredits'\n",
    "\n",
    "        # 하위 홈페이지를 selenium으로 열기\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(page)\n",
    "        \n",
    "        # more 열기\n",
    "        for x in range(20):\n",
    "            try:\n",
    "                more_link = WebDriverWait(driver, 1).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'div.sc-f65f65be-0.bBlII > ul > div > span.ipc-see-more'))\n",
    "                )\n",
    "                more_link.click()\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        time.sleep(8)\n",
    "        \n",
    "        # 현재 페이지 내의 정보를 저장\n",
    "        raw = driver.page_source\n",
    "        \n",
    "        # BeautifulSoup을 이용하여 html 파싱\n",
    "        soup = BeautifulSoup(raw, 'html.parser')\n",
    "            \n",
    "        \n",
    "        # 제작사\n",
    "        try:\n",
    "            production_list = soup.select('#__next > main > div > section > div > section > div > div.sc-978e9339-1.ihWZgK.ipc-page-grid__item.ipc-page-grid__item--span-2 > section:nth-child(2)')[0]\n",
    "            production_text = production_list.find('span', string='Production Companies').get_text(strip=True)\n",
    "\n",
    "            if production_text == 'Production Companies':\n",
    "                production_companies = production_list.find_all(class_=\"ipc-metadata-list-item__label--link\")\n",
    "                for company in production_companies:\n",
    "                    production_set.add(company.text)\n",
    "                drama.at[idx, 'production'] = list(production_set)\n",
    "\n",
    "        except:\n",
    "            drama.at[idx, 'production'] = np.NaN\n",
    "\n",
    "        # 배급사\n",
    "\n",
    "        # 두 번째 항목인지 확인\n",
    "        try:\n",
    "            distributor_list = soup.select('#__next > main > div > section > div > section > div > div.sc-978e9339-1.ihWZgK.ipc-page-grid__item.ipc-page-grid__item--span-2 > section:nth-child(3)')[0]\n",
    "            distributor_text = distributor_list.find('span', string='Distributors').get_text(strip=True)\n",
    "\n",
    "            if distributor_text == 'Distributors':\n",
    "                distributor_companies = distributor_list.find_all(class_=\"ipc-metadata-list-item__label--link\")\n",
    "                for company in distributor_companies:\n",
    "                    distributor_set.add(company.text)\n",
    "                drama.at[idx, 'distributor'] = list(distributor_set)\n",
    "\n",
    "        except:\n",
    "\n",
    "            # 첫 번째 항목인지 확인\n",
    "            try:\n",
    "                distributor_list = soup.select('#__next > main > div > section > div > section > div > div.sc-978e9339-1.ihWZgK.ipc-page-grid__item.ipc-page-grid__item--span-2 > section:nth-child(2)')[0]\n",
    "                distributor_text = distributor_list.find('span', string='Distributors').get_text(strip=True)\n",
    "\n",
    "                if distributor_text == 'Distributors':\n",
    "                    distributor_companies = distributor_list.find_all(class_=\"ipc-metadata-list-item__label--link\")\n",
    "                    for company in distributor_companies:\n",
    "                        distributor_set.add(company.text)\n",
    "                    drama.at[idx, 'distributor'] = list(distributor_set)\n",
    "                else:\n",
    "                    distributor_list = soup.select('#__next > main > div > section > div > section > div > div.sc-978e9339-1.ihWZgK.ipc-page-grid__item.ipc-page-grid__item--span-2 > section:nth-child(4)')[0]\n",
    "                    distributor_text = distributor_list.find('span', string='Distributors').get_text(strip=True)\n",
    "\n",
    "                    if distributor_text == 'Distributors':\n",
    "                        distributor_companies = distributor_list.find_all(class_=\"ipc-metadata-list-item__label--link\")\n",
    "                        for company in distributor_companies:\n",
    "                            distributor_set.add(company.text)\n",
    "                        drama.at[idx, 'distributor'] = list(distributor_set)\n",
    "            \n",
    "            # 세 번째 항목인지 확인\n",
    "            except:\n",
    "                distributor_list = soup.select('#__next > main > div > section > div > section > div > div.sc-978e9339-1.ihWZgK.ipc-page-grid__item.ipc-page-grid__item--span-2 > section:nth-child(4)')[0]\n",
    "                distributor_text = distributor_list.find('span', string='Distributors').get_text(strip=True)\n",
    "\n",
    "                if distributor_text == 'Distributors':\n",
    "                    distributor_companies = distributor_list.find_all(class_=\"ipc-metadata-list-item__label--link\")\n",
    "                    for company in distributor_companies:\n",
    "                        distributor_set.add(company.text)\n",
    "                    drama.at[idx, 'distributor'] = list(distributor_set)\n",
    "                else:\n",
    "                    distributor_list = soup.select('#__next > main > div > section > div > section > div > div.sc-978e9339-1.ihWZgK.ipc-page-grid__item.ipc-page-grid__item--span-2 > section:nth-child(2)')[0]\n",
    "                    distributor_text = distributor_list.find('span', string='Distributors').get_text(strip=True)\n",
    "\n",
    "                    if distributor_text == 'Distributors':\n",
    "                        distributor_companies = distributor_list.find_all(class_=\"ipc-metadata-list-item__label--link\")\n",
    "                        for company in distributor_companies:\n",
    "                            distributor_set.add(company.text)\n",
    "                        drama.at[idx, 'distributor'] = list(distributor_set)\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "    except:\n",
    "        drama.at[idx, 'production'] = np.NaN\n",
    "        drama.at[idx, 'distributor'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. 드라마 감독, 작가, 배우 페이지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# Selenium 웹 드라이버 설정\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        # cast & crew 페이지 URL 생성\n",
    "        fullcredits_url = url + 'fullcredits/?ref_=tt_ql_1'\n",
    "        \n",
    "        # cast & crew 페이지로 이동\n",
    "        driver.get(fullcredits_url)\n",
    "\n",
    "        # 페이지가 로드될 때까지 기다림\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h1')))\n",
    "\n",
    "        # 페이지 소스 가져오기\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # 감독 정보 추출\n",
    "        directors = []\n",
    "        director_elements = soup.select('#director + .simpleCreditsTable a')\n",
    "        for elem in director_elements:\n",
    "            directors.append(elem.text.strip())\n",
    "\n",
    "        # 작가 정보 추출\n",
    "        writers = []\n",
    "        writer_elements = soup.select('#writer + .simpleCreditsTable a')\n",
    "        for elem in writer_elements:\n",
    "            writers.append(elem.text.strip())\n",
    "\n",
    "        # 배우 정보 추출 (10명까지)\n",
    "        actors = []\n",
    "        actor_elements = soup.select('table.cast_list tr .primary_photo + td a')\n",
    "        for elem in actor_elements[:10]:\n",
    "            actors.append(elem.text.strip())\n",
    "\n",
    "        # 중복 제거 및 결과를 리스트에 추가\n",
    "        data.append({\n",
    "            'IMDb_URL': url,\n",
    "            'title': df.loc[df['IMDb_URL'] == url, 'title'].values[0],  # 제목도 추가\n",
    "            'fullcredits_url': fullcredits_url,\n",
    "            'director': ', '.join(sorted(set(directors))) if directors else np.nan,\n",
    "            'writer': ', '.join(sorted(set(writers))) if writers else np.nan,\n",
    "            'actor': ', '.join(actors) if actors else np.nan\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process URL {url}: {e}\")\n",
    "        data.append({\n",
    "            'IMDb_URL': url,\n",
    "            'title': df.loc[df['IMDb_URL'] == url, 'title'].values[0] if not df.loc[df['IMDb_URL'] == url].empty else np.nan,  # 제목도 추가\n",
    "            'fullcredits_url': np.nan,\n",
    "            'director': np.nan,\n",
    "            'writer': np.nan,\n",
    "            'actor': np.nan\n",
    "        })\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. 드라마 시즌별 페이지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()  # Make sure you have the appropriate WebDriver installed\n",
    "action = ActionChains(driver)\n",
    "\n",
    "for idx, url in tqdm(df.iterrows()):\n",
    "    if type(url['imdb_url']) == float:\n",
    "        continue\n",
    "    else:\n",
    "        driver.get(url['imdb_url'] + 'episodes')\n",
    "    \n",
    "    # 시즌 개수 추출\n",
    "    season = driver.find_elements(By.CSS_SELECTOR,'a[data-testid=\"tab-season-entry\"]')\n",
    "\n",
    "    for s in range(len(season)):\n",
    "        time.sleep(3)\n",
    "        \n",
    "        years = []\n",
    "        score_dict = {}\n",
    "\n",
    "        # 에피소드별 정보 추출\n",
    "        episode_score = driver.find_elements(By.CSS_SELECTOR, '#__next > main > div > section > div > section > div > div > section:nth-child(2) > section > article')\n",
    "\n",
    "        \n",
    "        # 해당 시즌 에피소드의 연도만 추출\n",
    "        for i in episode_score:\n",
    "            y = 0\n",
    "            for j in re.split('(\\d\\d\\d\\d)', i.text):\n",
    "                if len(j) == 4:\n",
    "                    years.append(j)\n",
    "                    break\n",
    "        \n",
    "        if '2024' in years:\n",
    "            break\n",
    "       \n",
    "        for row in episode_score:\n",
    "            data = row.text\n",
    "            data = data.split('\\n')\n",
    "            \n",
    "            for m in data:\n",
    "                if any(month in m[0:9] for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']):\n",
    "                    date_item = m\n",
    "                    print(date_item)\n",
    "                    break\n",
    "\n",
    "            for n in data:\n",
    "                if n[0] == 'S':\n",
    "                    season_episode_num = n.split(' ∙ ')[0]\n",
    "                    break\n",
    "            \n",
    "            try:\n",
    "                if data[-1] == 'Watch options':\n",
    "                    score = float(data[-5])\n",
    "                    vote_num = data[-3].strip()[1:-1]\n",
    "                    if vote_num[-1] == 'k':\n",
    "                        vote_num = int(vote_num[:-1]) * 1000\n",
    "                    else:\n",
    "                        vote_num = int(vote_num[:-1])\n",
    "                    if len(score) > 3:\n",
    "                        column = 'season_' + str(s + 1)\n",
    "                        df.at[idx, column] = '평점 없음'\n",
    "                        break\n",
    "                    score_dict[season_episode_num] = [score, vote_num,date_item]\n",
    "                else:\n",
    "                    score = float(data[-4])\n",
    "                    vote_num = data[-2].strip()[1:-1]\n",
    "                    if vote_num[-1] == 'k':\n",
    "                        vote_num = int(vote_num[:-1]) * 1000\n",
    "                    else:\n",
    "                        vote_num = int(vote_num[:-1])\n",
    "                    if len(score) > 3:\n",
    "                        column = 'season_' + str(s + 1)\n",
    "                        df.at[idx, column] = '평점 없음'\n",
    "                        break\n",
    "                    score_dict[season_episode_num] = [score, vote_num,date_item]\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        if len(score_dict.values()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            column = 'season_' + str(s + 1)\n",
    "            df.at[idx, column] = score_dict\n",
    "\n",
    "        if len(df.at[idx, column]) == 1:\n",
    "            df.at[idx, column] = '확인 필요'\n",
    "\n",
    "        if len(score) > 3:\n",
    "            df.at[idx, column] = '평점 없음'\n",
    "\n",
    "        episode_score = 0\n",
    "\n",
    "        print(score_dict)\n",
    "\n",
    "        if s < len(season) - 1:\n",
    "            some_tag = driver.find_element(By.CSS_SELECTOR, '#next-season-btn > svg')\n",
    "            action.move_to_element(some_tag).perform()\n",
    "            driver.find_element(By.CSS_SELECTOR, '#next-season-btn > svg').click()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인 필요 컬럼 추출\n",
    "\n",
    "for idx, row in df.loc[:,'season_1':'season_17'].iterrows():\n",
    "    for i in row:\n",
    "        if i == '확인 필요':\n",
    "            print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시즌 넘버가 잘못 지정된 밸류 추출\n",
    "\n",
    "for idx, row in df.loc[:,'season_1':'season_17'].iterrows():\n",
    "    s = 1\n",
    "    for j in row:\n",
    "        if type(j) == float:\n",
    "            break\n",
    "        elif j == None:\n",
    "            continue\n",
    "        elif type(j) != str:\n",
    "            for i in j.keys():\n",
    "                if 'S'+str(s) == i.split('.')[0]:\n",
    "                    continue\n",
    "                elif 'S'+str(s) != i.split('.')[0]:\n",
    "                    print(idx)\n",
    "                    break\n",
    "        s+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열이 들어간 밸류 추출\n",
    "\n",
    "for idx, row in df.loc[:,'season_1':'season_17'].iterrows():\n",
    "    for j in row:\n",
    "        if type(j) == float:\n",
    "            break\n",
    "        if type(j) != str:\n",
    "            for i in j.values():\n",
    "                if len(i[0]) > 3:\n",
    "                    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. 드라마 수상, 노미네이션 경력 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDb URL 리스트 추출\n",
    "urls = df['imdb_url'].tolist()\n",
    "\n",
    "# Selenium 웹 드라이버 설정\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        # IMDb 페이지로 이동\n",
    "        driver.get(url)\n",
    "\n",
    "        # 페이지가 로드될 때까지 기다림\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div/section[1]/div/section/div/div[1]/section[1]/div/ul/li/div/ul/li/span')))\n",
    "\n",
    "        # Wins와 Nominations 크롤링\n",
    "        wins = 0\n",
    "        nominations = 0\n",
    "\n",
    "        try:\n",
    "            # Wins와 Nominations 텍스트 가져오기\n",
    "            win_nom_elements = driver.find_elements(By.XPATH, '//*[@id=\"__next\"]/main/div/section[1]/div/section/div/div[1]/section[1]/div/ul/li/div/ul/li/span')\n",
    "\n",
    "            # Wins와 Nominations 추출\n",
    "            for element in win_nom_elements:\n",
    "                text = element.text\n",
    "                win_match = re.search(r'(\\d+)\\s+win', text)\n",
    "                nom_match = re.search(r'(\\d+)\\s+nomination', text)\n",
    "                if win_match:\n",
    "                    wins = int(win_match.group(1))\n",
    "                if nom_match:\n",
    "                    nominations = int(nom_match.group(1))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract wins and nominations: {e}\")\n",
    "\n",
    "        data.append({\n",
    "            'imdb_url': url,\n",
    "            'wins': wins,\n",
    "            'nominations': nominations\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract wins and nominations for URL {url}: {e}\")\n",
    "        data.append({\n",
    "            'imdb_url': url,\n",
    "            'wins': 0,\n",
    "            'nominations': 0\n",
    "        })\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
